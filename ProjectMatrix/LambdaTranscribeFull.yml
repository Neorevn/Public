AWSTemplateFormatVersion: "2010-09-09"
Description: "Full stack to handle MP3 transcription with monolithic Lambda, DynamoDB, S3 lifecycle, and CloudWatch/SNS monitoring. (v2.2 features, v2.12 CORS, cfnresponse fix)"
#Created by: Neorevn V3.5

Parameters:
  TranscriptionBucketName:
    Type: String
    Description: "Name of the bucket for uploads and transcription output. MUST BE GLOBALLY UNIQUE."
    Default: "Your-transcription-bucket" # Please change this
  NotificationEmail:
    Type: String
    Description: "Email address to receive notifications for alarms."
    Default: "YourEmail" # Please change this 
  CORSOrigin:
    Type: String
    Description: "CORS origin for API Gateway. Set to '*' for all origins or specify a domain."
    Default: "YourDomain" # Please change this
  UploadsExpirationDays:
    Type: Number
    Description: "Number of days after which to delete files from the 'uploads/' prefix."
    Default: 1
  JsonTranscriptsExpirationDays:
    Type: Number
    Description: "Number of days after which to delete intermediate JSON transcripts from 'transcripts/json/'."
    Default: 1
  TextTranscriptsExpirationDays:
    Type: Number
    Description: "Number of days after which to delete final text transcripts from 'transcripts/text/'."
    Default: 1
  DeploymentTimestamp: 
    Type: String
    Description: "Timestamp or unique string to force API deployment. Change this value to redeploy API."
    Default: "YourStackVersion"


Resources:
  # S3 Bucket for uploads and transcripts
  TranscriptionBucket:
    Type: AWS::S3::Bucket
    DependsOn:
      - AllowS3ToInvokeLambda
    Properties:
      BucketName: !Ref TranscriptionBucketName
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders: ["*"]
            AllowedMethods: [GET, PUT, POST, DELETE, HEAD]
            AllowedOrigins: 
              - !Ref CORSOrigin
            ExposedHeaders: [ETag]
            MaxAge: 3000
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: prefix
                    Value: uploads/
                  - Name: suffix
                    Value: .mp3
            Function: !GetAtt TranscribeAudioFunction.Arn
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldUploads
            Status: Enabled
            Prefix: uploads/
            ExpirationInDays: !Ref UploadsExpirationDays
          - Id: DeleteOldJsonTranscripts
            Status: Enabled
            Prefix: transcripts/json/
            ExpirationInDays: !Ref JsonTranscriptsExpirationDays
          - Id: DeleteOldTextTranscripts
            Status: Enabled
            Prefix: transcripts/text/
            ExpirationInDays: !Ref TextTranscriptsExpirationDays

  # Custom Resource to create initial "folders" (object prefixes) in the S3 bucket
  BucketFolders:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt CreateBucketFoldersFunction.Arn
      BucketName: !Ref TranscriptionBucketName
      Folders:
        - uploads/
        - transcripts/json/
        - transcripts/text/
    DependsOn: TranscriptionBucket

  CreateBucketFoldersFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-CreateBucketFolders"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt CreateBucketFoldersRole.Arn
      Timeout: 60
      MemorySize: 128
      Code:
        ZipFile: |
          # --- BEGIN cfnresponse.py content (copied verbatim) ---
          import urllib3
          import json

          # These constants and the send function will be globally available to the handler code below
          SUCCESS = "SUCCESS"
          FAILED = "FAILED"

          http = urllib3.PoolManager()

          def send(event, context, responseStatus, responseData, physicalResourceId=None, noEcho=False, reason=None):
              responseUrl = event['ResponseURL']

              # Using print for logging as in many cfnresponse examples
              # These logs will appear in CloudWatch under the Lambda function's log group
              print(f"CFNRESPONSE - Response URL: {responseUrl}")
              # print(f"CFNRESPONSE - Event: {json.dumps(event)}") # Uncomment for very detailed event logging if needed

              responseBody = {
                  'Status': responseStatus,
                  'Reason': reason or "See the details in CloudWatch Log Stream: " + context.log_stream_name,
                  'PhysicalResourceId': physicalResourceId or context.log_stream_name,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'NoEcho': noEcho,
                  'Data': responseData
              }

              json_responseBody = json.dumps(responseBody)

              print(f"CFNRESPONSE - Response body: {json_responseBody}")

              headers = {
                  'content-type': '',
                  'content-length': str(len(json_responseBody))
              }

              try:
                  response = http.request('PUT', responseUrl, headers=headers, body=json_responseBody)
                  print(f"CFNRESPONSE - Status code from S3 presigned URL: {response.status}")
              except Exception as e:
                  print(f"CFNRESPONSE - send(..) failed executing http.request(..): {e}")
                  # If send fails, the custom resource will time out in CloudFormation.
                  # It's critical this part works. Raising the exception helps surface it in Lambda logs.
                  raise
          # --- END cfnresponse.py content ---

          # --- BEGIN Original Handler Code (modified for bundled cfnresponse) ---
          import boto3
          import logging # logging module is still used for the handler's specific logs

          s3 = boto3.client('s3')
          logger = logging.getLogger("S3FolderCreatorHandler")
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context):
              logger.info(f"S3FolderCreatorHandler - Received event: {json.dumps(event)}")
              
              props = event.get('ResourceProperties', {}) # Default to empty dict if not present
              bucket_name = props.get('BucketName')
              folders_to_create = props.get('Folders', []) # Default to empty list
              
              # Use the globally defined SUCCESS, FAILED from the vendored cfnresponse code
              response_status = SUCCESS # Default to SUCCESS
              response_data = {}
              
              # Determine PhysicalResourceId:
              # For 'Create', generate a new one.
              # For 'Update' or 'Delete', use the one from the event.
              physical_resource_id = event.get('PhysicalResourceId')
              if event['RequestType'] == 'Create':
                  if not physical_resource_id: # Should always be true for 'Create'
                      # Create a unique ID. Using parts of log stream name is common.
                      physical_resource_id = f"s3-folders-{bucket_name or 'unknownbucket'}-{context.log_stream_name.split('/')[-1]}"
                  logger.info(f"S3FolderCreatorHandler - CREATE request. Generated PhysicalResourceId: {physical_resource_id}")
              else: # Update or Delete
                  if not physical_resource_id:
                      # This case should ideally not happen for Update/Delete if CFN is working correctly.
                      logger.warning("S3FolderCreatorHandler - PhysicalResourceId not found in event for Update/Delete. Using log stream name as fallback.")
                      physical_resource_id = context.log_stream_name 
                  logger.info(f"S3FolderCreatorHandler - {event['RequestType']} request. Using PhysicalResourceId: {physical_resource_id}")

              try:
                  if not bucket_name:
                      raise ValueError("ResourceProperties must include 'BucketName'.")

                  request_type = event['RequestType']
                  logger.info(f"S3FolderCreatorHandler - Processing RequestType: {request_type} for bucket: {bucket_name}")

                  if request_type == 'Create' or request_type == 'Update':
                      if not folders_to_create:
                          logger.info("S3FolderCreatorHandler - No 'Folders' specified to create/update.")
                      else:
                          for folder_key in folders_to_create:
                              # Ensure the "folder" key ends with a slash
                              if not folder_key.endswith('/'):
                                  folder_key_with_slash = folder_key + '/'
                              else:
                                  folder_key_with_slash = folder_key
                              
                              logger.info(f"S3FolderCreatorHandler - Ensuring folder (prefix object) exists: s3://{bucket_name}/{folder_key_with_slash}")
                              s3.put_object(Bucket=bucket_name, Key=folder_key_with_slash, Body='') # Create a 0-byte object
                          logger.info(f"S3FolderCreatorHandler - Successfully processed all specified folders for bucket: {bucket_name}")
                  
                  elif request_type == 'Delete':
                      logger.info(f"S3FolderCreatorHandler - RequestType 'Delete'. PhysicalResourceId: {physical_resource_id}.")
                      logger.info("S3FolderCreatorHandler - No explicit S3 delete action taken for the folder prefix objects by this custom resource. S3 prefixes are not deleted like directories.")
                      # Note: The 0-byte objects created by this resource to represent folders will remain
                      # unless explicitly deleted or managed by a lifecycle policy.
                      # If you need to delete these specific 0-byte objects on stack deletion,
                      # you would add s3.delete_object calls here for each key in props.get('OldResourceProperties', {}).get('Folders', []).
                      # However, this is often not required as S3 "folders" are just prefixes.
                  
                  else:
                      logger.warning(f"S3FolderCreatorHandler - Unsupported RequestType: {request_type}. No action taken.")
                      # Still send SUCCESS as no error occurred in processing the unknown type.

              except Exception as e:
                  logger.error(f"S3FolderCreatorHandler - Unhandled exception: {str(e)}", exc_info=True)
                  response_status = FAILED # Use the globally defined FAILED
                  response_data['Error'] = f"An error occurred: {str(e)}" # Provide error info in response
                  # The 'reason' for the cfnresponse.send will be automatically populated with log stream info
              
              finally:
                  # Use the send function defined above, NOT cfnresponse.send
                  send(event, context, response_status, response_data, physical_resource_id)
                  logger.info("S3FolderCreatorHandler - Response sent.")
          # --- END Original Handler Code (modified) ---
    DependsOn: CreateBucketFoldersRole

  CreateBucketFoldersRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: {Version: "2012-10-17", Statement: [{Effect: Allow, Principal: {Service: lambda.amazonaws.com}, Action: sts:AssumeRole}]}
      Path: "/"
      Policies:
        - PolicyName: "CreateBucketFoldersPolicy"
          PolicyDocument: {Version: "2012-10-17", Statement: [
              {Effect: Allow, Action: [s3:PutObject, s3:DeleteObject], Resource: !Sub "arn:aws:s3:::${TranscriptionBucketName}/*"},
              {Effect: Allow, Action: [s3:ListBucket], Resource: !Sub "arn:aws:s3:::${TranscriptionBucketName}"},
              {Effect: Allow, Action: ["logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents"], Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::StackName}-CreateBucketFolders*:*"} # Added wildcard for log stream
            ]}

  # DynamoDB Table for Transcription Metadata
  TranscriptionMetadataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub "${AWS::StackName}-TranscriptionMetadata"
      AttributeDefinitions:
        - AttributeName: "jobId"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "jobId"
          KeyType: "HASH"
      BillingMode: PAY_PER_REQUEST

  # IAM Role for the TranscribeAudioFunction, UploadFileFunction, and FetchTranscriptFunction Lambdas
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: {Version: "2012-10-17", Statement: [{Effect: Allow, Principal: {Service: lambda.amazonaws.com}, Action: "sts:AssumeRole"}]}
      Path: "/"
      Policies:
        - PolicyName: "LambdaCoreExecutionPolicy"
          PolicyDocument: {Version: "2012-10-17", Statement: [{Effect: Allow, Action: ["logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents"], Resource: "*"}]}
        - PolicyName: "S3AccessPolicy"
          PolicyDocument: {Version: "2012-10-17", Statement: [
              {Effect: Allow, Action: ["s3:GetObject", "s3:PutObject", "s3:DeleteObject"], Resource: !Sub "arn:aws:s3:::${TranscriptionBucketName}/*"},
              {Effect: Allow, Action: ["s3:ListBucket"], Resource: !Sub "arn:aws:s3:::${TranscriptionBucketName}"}
            ]}
        - PolicyName: "TranscribeAccessPolicy"
          PolicyDocument: {Version: "2012-10-17", Statement: [{Effect: Allow, Action: ["transcribe:StartTranscriptionJob", "transcribe:GetTranscriptionJob", "transcribe:DeleteTranscriptionJob"], Resource: "*"}]}
        - PolicyName: "DynamoDBAccessPolicy"
          PolicyDocument: {Version: "2012-10-17", Statement: [{Effect: Allow, Action: ["dynamodb:PutItem", "dynamodb:UpdateItem", "dynamodb:GetItem"], Resource: !GetAtt TranscriptionMetadataTable.Arn}]}

  # Lambda Function to initiate and monitor transcription jobs (monolithic)
  TranscribeAudioFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-TranscribeAudio"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          OUTPUT_BUCKET_NAME: !Ref TranscriptionBucketName
          DYNAMODB_TABLE_NAME: !Ref TranscriptionMetadataTable
      Code:
        ZipFile: |
          import boto3, urllib.parse, os, time, json, logging, re
          from datetime import datetime, timezone
          s3 = boto3.client('s3'); transcribe = boto3.client('transcribe'); dynamodb = boto3.resource('dynamodb')
          OUTPUT_BUCKET_NAME = os.environ['OUTPUT_BUCKET_NAME']; DYNAMODB_TABLE_NAME = os.environ['DYNAMODB_TABLE_NAME']
          metadata_table = dynamodb.Table(DYNAMODB_TABLE_NAME)
          UPLOAD_PREFIX = "uploads"; TRANSCRIPT_JSON_PREFIX = "transcripts/json"; TRANSCRIPT_TEXT_PREFIX = "transcripts/text"
          logger = logging.getLogger(); logger.setLevel(logging.INFO)
          if not logging.getLogger().handlers: logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] (%(name)s) %(message)s')


          def update_dynamodb(job_id, status, original_file_name=None, s3_upload_key=None, s3_transcript_json_path=None, s3_transcript_text_path=None, error_message=None, upload_ts=None, completion_ts=None):
              try:
                  timestamp = datetime.now(timezone.utc).isoformat()
                  item_updates = {'status': status, 'lastUpdated': timestamp}
                  if original_file_name: item_updates['originalFileName'] = original_file_name
                  if s3_upload_key: item_updates['s3UploadKey'] = s3_upload_key
                  if upload_ts: item_updates['uploadTimestamp'] = upload_ts # Only on initial
                  if s3_transcript_json_path: item_updates['s3TranscriptJsonPath'] = s3_transcript_json_path
                  if s3_transcript_text_path: item_updates['s3TranscriptTextPath'] = s3_transcript_text_path
                  if error_message: item_updates['errorMessage'] = error_message
                  if completion_ts: item_updates['completionTimestamp'] = completion_ts
                  
                  if status == "PROCESSING" and upload_ts: # Initial record
                      item_to_put = {'jobId': job_id, **item_updates}
                      metadata_table.put_item(Item=item_to_put)
                      logger.info(f"Initial metadata for job {job_id}: {item_to_put}")
                  else: # Update existing record
                      update_expression_parts = ["SET #s_key = :s_val", "#lu_key = :lu_val"]
                      expression_attribute_names = {'#s_key': 'status', '#lu_key': 'lastUpdated'}
                      expression_attribute_values = {':s_val': status, ':lu_val': timestamp}
                      
                      for key, value in item_updates.items():
                          if key not in ['status', 'lastUpdated']: # Already handled
                              attr_name_placeholder = f"#{key}_key"
                              attr_val_placeholder = f":{key}_val"
                              update_expression_parts.append(f"{attr_name_placeholder} = {attr_val_placeholder}")
                              expression_attribute_names[attr_name_placeholder] = key
                              expression_attribute_values[attr_val_placeholder] = value
                      
                      update_expression = ", ".join(update_expression_parts)
                      logger.info(f"Updating DynamoDB for job {job_id}. UpdateExpression: {update_expression}, Names: {expression_attribute_names}, Values: {expression_attribute_values}")
                      metadata_table.update_item(Key={'jobId': job_id}, UpdateExpression=update_expression, ExpressionAttributeNames=expression_attribute_names, ExpressionAttributeValues=expression_attribute_values)
                  logger.info(f"DynamoDB updated for job {job_id} with status {status}")
              except Exception as e:
                  logger.error(f"Error updating DynamoDB for job {job_id}: {e}", exc_info=True)

          def lambda_handler(event, context):
              logger.info(f"S3 event: {json.dumps(event)}")
              if not event.get('Records'): return {'statusCode': 200, 'body': 'No S3 records.'}
              bucket_name = event['Records'][0]['s3']['bucket']['name']
              file_key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
              upload_timestamp = datetime.now(timezone.utc).isoformat()
              if not file_key.lower().startswith(f"{UPLOAD_PREFIX}/") or not file_key.lower().endswith('.mp3'):
                  logger.info(f"Skipping non-MP3 or non-upload file: {file_key}")
                  return {'statusCode': 200, 'body': 'Not an MP3 in uploads/.'}
              base_filename = os.path.basename(file_key)
              sanitized_part = re.sub(r'[^0-9a-zA-Z._-]', '_', os.path.splitext(base_filename)[0])
              job_name = f"lambda-transcribe-{sanitized_part[:150]}-{int(time.time())}"
              job_name = re.sub(r'__+', '_', job_name).strip('_') or f"lambda-transcribe-job-{int(time.time())}"
              media_uri = f"s3://{bucket_name}/{file_key}"
              s3_upload_key_path = f"s3://{bucket_name}/{file_key}"
              
              update_dynamodb(job_id=job_name, status="PROCESSING", original_file_name=base_filename, s3_upload_key=s3_upload_key_path, upload_ts=upload_timestamp)
              output_json_key = f"{TRANSCRIPT_JSON_PREFIX}/{base_filename}.json"
              s3_transcript_json_full_path = f"s3://{OUTPUT_BUCKET_NAME}/{output_json_key}"
              try:
                  transcribe.start_transcription_job(TranscriptionJobName=job_name, Media={'MediaFileUri': media_uri}, OutputBucketName=OUTPUT_BUCKET_NAME, OutputKey=output_json_key, MediaFormat='mp3', LanguageCode='en-US')
                  logger.info(f"Started job: {job_name}")
                  update_dynamodb(job_id=job_name, status="TRANSCRIBE_JOB_STARTED", s3_transcript_json_path=s3_transcript_json_full_path)
                  
                  polling_start_time = time.time()
                  while True:
                      if (time.time() - polling_start_time) > 270: # 4.5 min polling, 30s buffer
                          logger.warning(f"Lambda timeout approaching for {job_name}. Aborting polling.")
                          update_dynamodb(job_id=job_name, status="PROCESSING_LAMBDA_TIMEOUT")
                          return {'statusCode': 202, 'body': f"Job {job_name} polling aborted (timeout)."}
                      job_details = transcribe.get_transcription_job(TranscriptionJobName=job_name)
                      status = job_details['TranscriptionJob']['TranscriptionJobStatus']
                      logger.info(f"Job {job_name} status: {status}")
                      if status in ['COMPLETED', 'FAILED']: break
                      time.sleep(15)
                  
                  completion_timestamp = datetime.now(timezone.utc).isoformat()
                  if status == 'FAILED':
                      failure_reason = job_details['TranscriptionJob'].get('FailureReason', 'Unknown')
                      logger.error(f"Job {job_name} failed for {file_key}. Reason: {failure_reason}")
                      update_dynamodb(job_id=job_name, status="FAILED", error_message=failure_reason, completion_ts=completion_timestamp)
                      transcribe.delete_transcription_job(TranscriptionJobName=job_name)
                      # s3.delete_object(Bucket=bucket_name, Key=file_key) # Optionally delete original on failure
                      return {'statusCode': 500, 'body': f"Transcription failed: {failure_reason}"}

                  result = s3.get_object(Bucket=OUTPUT_BUCKET_NAME, Key=output_json_key)
                  transcript_data = json.loads(result['Body'].read().decode('utf-8'))
                  text_content = transcript_data['results']['transcripts'][0]['transcript']
                  text_file_basename = f"{os.path.splitext(base_filename)[0]}.txt"
                  text_file_key = f"{TRANSCRIPT_TEXT_PREFIX}/{text_file_basename}"
                  s3_transcript_text_full_path = f"s3://{OUTPUT_BUCKET_NAME}/{text_file_key}"
                  s3.put_object(Bucket=OUTPUT_BUCKET_NAME, Key=text_file_key, Body=text_content.encode('utf-8'), ContentType='text/plain')
                  logger.info(f"Saved text to {s3_transcript_text_full_path}")
                  update_dynamodb(job_id=job_name, status="COMPLETED_SUCCESS", s3_transcript_text_path=s3_transcript_text_full_path, completion_ts=completion_timestamp)
                  
                  logger.info(f"Deleting JSON: {s3_transcript_json_full_path}"); s3.delete_object(Bucket=OUTPUT_BUCKET_NAME, Key=output_json_key)
                  logger.info(f"Deleting MP3: {s3_upload_key_path}"); s3.delete_object(Bucket=bucket_name, Key=file_key)
                  logger.info(f"Deleting Transcribe job: {job_name}"); transcribe.delete_transcription_job(TranscriptionJobName=job_name)
                  return {'statusCode': 200, 'body': f"Transcript for {file_key} complete. Job ID: {job_name}"}
              except Exception as e:
                  logger.error(f"Error processing {file_key} for job {job_name}: {e}", exc_info=True)
                  update_dynamodb(job_id=job_name, status="ERROR_LAMBDA", error_message=str(e), completion_ts=datetime.now(timezone.utc).isoformat())
                  return {'statusCode': 500, 'body': f"Error processing {file_key}: {str(e)}"}

  # Lambda Function for handling uploads via API Gateway
  UploadFileFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-UploadFile"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 30
      MemorySize: 128
      Environment:
        Variables:
          UPLOAD_BUCKET_NAME: !Ref TranscriptionBucketName
          UPLOAD_PREFIX: "uploads"
      Code:
        ZipFile: |
          import boto3, json, base64, os, logging, mimetypes
          s3 = boto3.client('s3'); UPLOAD_BUCKET_NAME = os.environ['UPLOAD_BUCKET_NAME']; UPLOAD_PREFIX = os.environ.get('UPLOAD_PREFIX', "uploads")
          logger = logging.getLogger(); logger.setLevel(logging.INFO)
          if not logging.getLogger().handlers: logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] (%(name)s) %(message)s')
          def lambda_handler(event, context):
              logger.info(f"Upload event headers: {event.get('headers')}")
              try:
                  if not event.get("body"): return {"statusCode": 400, "headers": {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, "body": json.dumps({"error": "Missing body"})}
                  body_str = event["body"];
                  if event.get("isBase64Encoded", False): body_str = base64.b64decode(body_str).decode('utf-8')
                  try: body = json.loads(body_str)
                  except json.JSONDecodeError: return {"statusCode": 400, "headers": {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, "body": json.dumps({"error": "Invalid JSON"})}
                  file_content_b64 = body.get("fileContent"); file_name = body.get("fileName")
                  if not file_content_b64 or not file_name: return {"statusCode": 400, "headers": {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, "body": json.dumps({"error": "Missing fileContent or fileName"})}
                  if not file_name.lower().endswith('.mp3'): return {"statusCode": 400, "headers": {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, "body": json.dumps({"error": "Only .mp3 supported"})}
                  s3_content_type, _ = mimetypes.guess_type(file_name)
                  if not s3_content_type or not s3_content_type.startswith('audio/mpeg'): s3_content_type = 'audio/mpeg'
                  try: file_content_bytes = base64.b64decode(file_content_b64)
                  except Exception: return {"statusCode": 400, "headers": {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, "body": json.dumps({"error": "Invalid Base64"})}
                  safe_file_name = os.path.basename(file_name); upload_key = f"{UPLOAD_PREFIX.strip('/')}/{safe_file_name}"
                  s3.put_object(Bucket=UPLOAD_BUCKET_NAME, Key=upload_key, Body=file_content_bytes, ContentType=s3_content_type)
                  logger.info(f"Uploaded {safe_file_name} to s3://{UPLOAD_BUCKET_NAME}/{upload_key}")
                  return {"statusCode": 200, "headers": {"Access-Control-Allow-Origin": "*", "Access-Control-Allow-Methods": "POST,OPTIONS", "Access-Control-Allow-Headers": "Content-Type,X-Amz-Date,Authorization,X-Api-Key", "Content-Type": "application/json"}, "body": json.dumps({"message": f"File {safe_file_name} uploaded. Transcription will be initiated."})}
              except Exception as e:
                  logger.error(f"Upload error: {e}", exc_info=True)
                  return {"statusCode": 500, "headers": {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, "body": json.dumps({"error": str(e)})}

  # Lambda Function to fetch transcribed text
  FetchTranscriptFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-FetchTranscript"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 60
      MemorySize: 128
      Environment:
        Variables:
          OUTPUT_BUCKET_NAME: !Ref TranscriptionBucketName
          TRANSCRIPT_TEXT_PREFIX: "transcripts/text"
          DYNAMODB_TABLE_NAME: !Ref TranscriptionMetadataTable 
      Code:
        ZipFile: |
          import boto3, json, urllib.parse, os, logging
          s3 = boto3.client('s3'); OUTPUT_BUCKET_NAME = os.environ['OUTPUT_BUCKET_NAME']; TRANSCRIPT_TEXT_PREFIX = os.environ['TRANSCRIPT_TEXT_PREFIX']
          logger = logging.getLogger(); logger.setLevel(logging.INFO)
          if not logging.getLogger().handlers: logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] (%(name)s) %(message)s')
          def lambda_handler(event, context):
              logger.info(f"Fetch event: {json.dumps(event)}")
              try:
                  if not event.get('pathParameters') or not event['pathParameters'].get('key'):
                      return {'statusCode': 400, 'headers': {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, 'body': json.dumps({'error': 'Missing transcript key'})}
                  requested_key_base = urllib.parse.unquote_plus(event['pathParameters']['key'])
                  transcript_s3_key = f"{TRANSCRIPT_TEXT_PREFIX.strip('/')}/{requested_key_base}.txt"
                  logger.info(f"Fetching s3://{OUTPUT_BUCKET_NAME}/{transcript_s3_key}")
                  try:
                      response = s3.get_object(Bucket=OUTPUT_BUCKET_NAME, Key=transcript_s3_key)
                      content = response['Body'].read().decode('utf-8')
                      return {'statusCode': 200, 'headers': {"Content-Type": "text/plain; charset=utf-8", "Access-Control-Allow-Origin": "*", "Access-Control-Allow-Methods": "GET,OPTIONS", "Access-Control-Allow-Headers": "Content-Type"}, 'body': content}
                  except s3.exceptions.NoSuchKey:
                      logger.warning(f"Transcript {transcript_s3_key} not found.")
                      return {'statusCode': 404, 'headers': {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, 'body': json.dumps({'error': f'Transcript for {requested_key_base}.txt not found or still processing.'})}
              except Exception as e:
                  logger.error(f"Fetch error for {requested_key_base if 'requested_key_base' in locals() else 'unknown'}: {e}", exc_info=True)
                  return {'statusCode': 500, 'headers': {"Access-Control-Allow-Origin": "*", "Content-Type": "application/json"}, 'body': json.dumps({'error': str(e)})}

  # Lambda Permission for S3 to invoke TranscribeAudioFunction
  AllowS3ToInvokeLambda:
    Type: AWS::Lambda::Permission
    DependsOn: TranscribeAudioFunction
    Properties:
      FunctionName: !GetAtt TranscribeAudioFunction.Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub arn:aws:s3:::${TranscriptionBucketName}

  # --- API Gateway ---
  UploadApi:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: !Sub "${AWS::StackName}-TranscriptionAPI-Reverted"
      Description: "API for MP3 upload and transcription retrieval (Reverted)."
      EndpointConfiguration: {Types: [REGIONAL]}

  UploadResource:
    Type: AWS::ApiGateway::Resource
    Properties: {ParentId: !GetAtt UploadApi.RootResourceId, PathPart: "upload", RestApiId: !Ref UploadApi}

  UploadMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref UploadApi
      ResourceId: !Ref UploadResource
      HttpMethod: POST
      AuthorizationType: NONE
      Integration: {IntegrationHttpMethod: POST, Type: AWS_PROXY, Uri: !Sub "arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${UploadFileFunction.Arn}/invocations"}
      MethodResponses:
        - StatusCode: "200"
          ResponseModels: {"application/json": !Ref EmptyModel}
          ResponseParameters: {'method.response.header.Access-Control-Allow-Origin': 'true'}
        - StatusCode: "400"
          ResponseModels: {"application/json": !Ref ErrorModel}
          ResponseParameters: {'method.response.header.Access-Control-Allow-Origin': 'true'}
        - StatusCode: "500"
          ResponseModels: {"application/json": !Ref ErrorModel}
          ResponseParameters: {'method.response.header.Access-Control-Allow-Origin': 'true'}

  UploadOptionsMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref UploadApi
      ResourceId: !Ref UploadResource
      HttpMethod: OPTIONS
      AuthorizationType: NONE
      Integration:
        Type: MOCK
        RequestTemplates: {"application/json": '{"statusCode": 200}'}
        IntegrationResponses:
          - StatusCode: "200"
            ResponseParameters:
              method.response.header.Access-Control-Allow-Origin: "'*'"
              method.response.header.Access-Control-Allow-Methods: "'POST,OPTIONS'"
              method.response.header.Access-Control-Allow-Headers: "'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Amz-User-Agent'"
            ResponseTemplates: {"application/json": "{}"}
      MethodResponses:
        - StatusCode: "200"
          ResponseParameters:
            method.response.header.Access-Control-Allow-Origin: "'*'"
            method.response.header.Access-Control-Allow-Methods: "'POST,OPTIONS'"
            method.response.header.Access-Control-Allow-Headers: "'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Amz-User-Agent'"
          ResponseModels: {"application/json": !Ref EmptyModel}

  FetchResource:
    Type: AWS::ApiGateway::Resource
    Properties: {ParentId: !GetAtt UploadApi.RootResourceId, PathPart: "transcripts", RestApiId: !Ref UploadApi}

  FetchItemResource:
    Type: AWS::ApiGateway::Resource
    Properties: {ParentId: !Ref FetchResource, PathPart: "{key}", RestApiId: !Ref UploadApi}

  FetchMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref UploadApi
      ResourceId: !Ref FetchItemResource
      HttpMethod: GET
      AuthorizationType: NONE
      Integration: {IntegrationHttpMethod: POST, Type: AWS_PROXY, Uri: !Sub "arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${FetchTranscriptFunction.Arn}/invocations"}
      MethodResponses:
        - StatusCode: "200"
          ResponseModels: {"text/plain": !Ref EmptyModel}
          ResponseParameters: {'method.response.header.Access-Control-Allow-Origin': 'true'}
        - StatusCode: "404"
          ResponseModels: {"application/json": !Ref ErrorModel}
          ResponseParameters: {'method.response.header.Access-Control-Allow-Origin': 'true'}
        - StatusCode: "500"
          ResponseModels: {"application/json": !Ref ErrorModel}
          ResponseParameters: {'method.response.header.Access-Control-Allow-Origin': 'true'}

  FetchOptionsMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref UploadApi
      ResourceId: !Ref FetchItemResource
      HttpMethod: OPTIONS
      AuthorizationType: NONE
      Integration:
        Type: MOCK
        RequestTemplates: {"application/json": '{"statusCode": 200}'}
        IntegrationResponses:
          - StatusCode: "200"
            ResponseParameters:
              method.response.header.Access-Control-Allow-Origin: "'*'"
              method.response.header.Access-Control-Allow-Methods: "'GET,OPTIONS'"
              method.response.header.Access-Control-Allow-Headers: "'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token'"
            ResponseTemplates: {"application/json": "{}"}
      MethodResponses:
        - StatusCode: "200"
          ResponseParameters:
            method.response.header.Access-Control-Allow-Origin: "'*'"
            method.response.header.Access-Control-Allow-Methods: "'GET,OPTIONS'"
            method.response.header.Access-Control-Allow-Headers: "'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token'"
          ResponseModels: {"application/json": !Ref EmptyModel}

  ErrorModel:
    Type: AWS::ApiGateway::Model
    Properties: {RestApiId: !Ref UploadApi, ContentType: "application/json", Name: !Sub "${AWS::StackName}Error", Schema: {type: "object",properties: {error: {type: "string"}}}}
  EmptyModel:
    Type: AWS::ApiGateway::Model
    Properties: {RestApiId: !Ref UploadApi, ContentType: "application/json", Name: !Sub "${AWS::StackName}Empty", Schema: {}}
  PlainTextModel:
    Type: AWS::ApiGateway::Model
    Properties: {RestApiId: !Ref UploadApi, ContentType: "text/plain", Name: !Sub "${AWS::StackName}PlainText", Schema: {type: "string"}}


  LambdaPermissionForUploadApi:
    Type: AWS::Lambda::Permission
    Properties: {FunctionName: !GetAtt UploadFileFunction.Arn, Action: lambda:InvokeFunction, Principal: apigateway.amazonaws.com, SourceArn: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${UploadApi}/*/POST/upload"}
  LambdaPermissionForFetchApi:
    Type: AWS::Lambda::Permission
    Properties: {FunctionName: !GetAtt FetchTranscriptFunction.Arn, Action: lambda:InvokeFunction, Principal: apigateway.amazonaws.com, SourceArn: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${UploadApi}/*/GET/transcripts/*"}

  ApiDeployment:
    Type: AWS::ApiGateway::Deployment
    DependsOn: [UploadMethod, UploadOptionsMethod, FetchMethod, FetchOptionsMethod, ErrorModel, EmptyModel, PlainTextModel]
    Properties: {RestApiId: !Ref UploadApi, StageName: "prod", Description: !Sub "Deployment Reverted - ${DeploymentTimestamp}"}

  # --- SNS and CloudWatch Alarms ---
  NotificationsSNSTopic:
    Type: AWS::SNS::Topic
    Properties: 
      DisplayName: !Sub "${AWS::StackName}-Notifications"
      TopicName: !Sub "${AWS::StackName}-NotificationsTopic"
      Subscription: [{Endpoint: !Ref NotificationEmail, Protocol: email}]
      
  SNSTopicPolicy:
    Type: AWS::SNS::TopicPolicy
    Properties: 
      PolicyDocument: 
        Statement: [{Effect: Allow, Principal: {Service: "cloudwatch.amazonaws.com"}, Action: "SNS:Publish", Resource: !Ref NotificationsSNSTopic}]
      Topics: [!Ref NotificationsSNSTopic]

  TranscribeLambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties: 
      AlarmName: !Sub "${AWS::StackName}-TranscribeAudioFunction-Errors"
      AlarmDescription: "Errors in TranscribeAudioFunction"
      Namespace: AWS/Lambda
      MetricName: Errors
      Dimensions: [{Name: FunctionName, Value: !Ref TranscribeAudioFunction}]
      Statistic: Sum
      Period: 120
      EvaluationPeriods: 1
      Threshold: 0
      ComparisonOperator: GreaterThanThreshold
      AlarmActions: [!Ref NotificationsSNSTopic]
      TreatMissingData: notBreaching
      
  UploadLambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties: 
      AlarmName: !Sub "${AWS::StackName}-UploadFileFunction-Errors"
      AlarmDescription: "Errors in UploadFileFunction"
      Namespace: AWS/Lambda
      MetricName: Errors
      Dimensions: [{Name: FunctionName, Value: !Ref UploadFileFunction}]
      Statistic: Sum
      Period: 60
      EvaluationPeriods: 1
      Threshold: 0
      ComparisonOperator: GreaterThanThreshold
      AlarmActions: [!Ref NotificationsSNSTopic]
      TreatMissingData: notBreaching
      
  FetchLambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties: 
      AlarmName: !Sub "${AWS::StackName}-FetchTranscriptFunction-Errors"
      AlarmDescription: "Errors in FetchTranscriptFunction"
      Namespace: AWS/Lambda
      MetricName: Errors
      Dimensions: [{Name: FunctionName, Value: !Ref FetchTranscriptFunction}]
      Statistic: Sum
      Period: 120
      EvaluationPeriods: 1
      Threshold: 0
      ComparisonOperator: GreaterThanThreshold
      AlarmActions: [!Ref NotificationsSNSTopic]
      TreatMissingData: notBreaching
      
  ApiGateway5xxErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties: 
      AlarmName: !Sub "${AWS::StackName}-API-5XX-Errors"
      AlarmDescription: "5XX server errors on TranscriptionAPI"
      Namespace: AWS/ApiGateway
      MetricName: "5XXError"
      Dimensions: [{Name: ApiName, Value: !Ref UploadApi}, {Name: Stage, Value: "prod"}]
      Statistic: Sum
      Period: 120
      EvaluationPeriods: 1
      Threshold: 0
      ComparisonOperator: GreaterThanThreshold
      AlarmActions: [!Ref NotificationsSNSTopic]
      TreatMissingData: notBreaching
      
  DynamoDBWriteThrottleEventsAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties: 
      AlarmName: !Sub "${TranscriptionMetadataTable}-WriteThrottleEvents"
      AlarmDescription: !Sub "Alarm when ${TranscriptionMetadataTable} write requests are throttled"
      Namespace: AWS/DynamoDB
      MetricName: WriteThrottleEvents
      Dimensions: [{Name: TableName, Value: !Ref TranscriptionMetadataTable}]
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      AlarmActions: [!Ref NotificationsSNSTopic]
      TreatMissingData: notBreaching

Outputs:
  ApiUrl:
    Description: "API Gateway base URL"
    Value: !Sub "https://${UploadApi}.execute-api.${AWS::Region}.amazonaws.com/prod"
    Export:
      Name: !Sub "${AWS::StackName}-ApiUrl"
  TranscriptionBucketNameOutput:
    Description: "S3 bucket name"
    Value: !Ref TranscriptionBucketName
    Export:
      Name: !Sub "${AWS::StackName}-BucketName"
  DynamoDBTableNameOutput:
    Description: "DynamoDB table name"
    Value: !Ref TranscriptionMetadataTable
    Export:
      Name: !Sub "${AWS::StackName}-DynamoDBTableName"
  SNSTopicArnOutput:
    Description: "SNS Topic ARN"
    Value: !Ref NotificationsSNSTopic
    Export:
      Name: !Sub "${AWS::StackName}-SNSTopicArn"
